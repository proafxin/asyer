\documentclass[manuscript,screen,review]{acmart}
%\AtBeginDocument{%
%	\providecommand\BibTeX{{%
%			\normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}

\setcopyright{acmcopyright}
\copyrightyear{2021}
\acmYear{2021}
\acmDOI{10.1145/1122445.1122456}

\usepackage{amsthm, amsmath, amssymb}
\usepackage[linesnumbered, ruled, vlined]{algorithm2e}
%\usepackage{csvsimple}
%\usepackage{biblatex}
\usepackage{pgffor}
%\usepackage{graphicx}

\title{\bfseries A Simple Yet Effective and Efficient Collaborative Filtering Based Recommendation System}
\author{Masum Billal}
\authornote{This research is done independently by the author, no grant or fund was accepted by the author for this work.}
\email{billalmasum93@gmail.com}
\orcid{0000-0002-2321-5456}
%\addbibresource{ref.bib}
\renewcommand{\shortauthors}{Billal, Masum}
\citestyle{acmauthoryear}
\begin{document}
	\begin{abstract}
		In order to improve both prediction accuracy and the time required to recommend items to a user based on collaborative filtering, a simple algorithm is proposed. The algorithm is different than traditional algorithms only in some simple steps.
	\end{abstract}
	\keywords{Recommender system, collaborative filtering, similarity measure}
	\maketitle
	\section{Introduction}
	A common problem with typical filtering based recommendation systems they usually require a lot of users similar to a user for prediction (such users are called \textbf{neighbors}). This heavily affects performance since the complexity gets incredibly high as well. \textcite{herlocker_konstan_borchers_riedl_1999} uses a memory based collaborative filtering technique that uses similarity between users to predict recommendations using KNN algorithm. This algorithm suffers from some problems such as when too few items are common between two users, predicting accurately becomes harder. \textcite{bell_koren_volinsky_2007} uses a model based learning which is inherently very expensive. \textcite{shen_wei_yang_2013} introduces a way to improve calculating similarity more accurately but they do not offer all that much improvement for the cost of learning similarity, so in this paper such improvement has been ignored and more traditional approaches have been used. \textcite{jamali_ester_2009}, \textcite{pavlov_pennock_2002} uses hybrid collaborative and content filtering or memory and model based algorithm for recommendation. While these works have their merits, most of them are complicated and offer little value to be used in practice. In this paper, we have showed that it may be possible to gain significantly better results by simply modifying traditional approaches a little bit using some principles which are mathematically sound. We will show that using our proposed modifications, we get a flat line in accuracy metric graphs much earlier.
	\section{Algorithm}
	Let us first describe the assumption problem that we want to solve. Let $\mathcal{M}$ be a set of movies and $\mathcal{U}$ be a set of users. We are given a set of triplets $(u,m,r)\in\mathcal{S}$ such that $u\in\mathcal{U},m\in\mathcal{U}$ and $1\leq r\leq 5$ which denotes the rating $r$ of movie $m$ given by user $u$. We are also given a set of pairs $(u,m)\in\mathcal{T}$ such that $u\in\mathcal{U},m\in\mathcal{M}$ and we want to predict the rating $r$ given by user $m$ to the movie $m$. Let $r_{um}$ be the rating of movie $m$ given by user $u$ and $C_{uv}$ be the set of items that both $u$ and $v$ rated. We use the \textit{Pearson Correlation Coefficient} (see \textcite{freedman2007statistics}) as the measure of correlation between user $u$ and $v$. However, we will use a normalized version of it for practical consideration as follows:
	\begin{align*}
		S_{uv}
		& = \dfrac{\sum_{m\in C_{uv}}(r_{um}-\bar{r_{u}})(r_{vm}-\bar{r_{v}})}{\sqrt{\sum_{m\in C_{uv}}(r_{um}-\bar{r_{u}})^{2}}\sqrt{\sum_{m\in C_{uv}}(r_{vm}-\bar{r_{v}})^{2}}}\\
		S_{uv}
		& \leftarrow \dfrac{S_{uv}+1}{2}
	\end{align*}
	This normalization is done using the fact that Pearson correlation coefficient $p_{uv}$ is always in the range $[-1,1]$. We call two users $u$ and $v$ similar if $S_{uv}\geq s$ for some positive real number $s$ such that $0\leq s\leq 1$. Typically, we want $s$ in the range $[.5, 1]$. For this paper, we will consider $s\in\{.7,.8,.9\}$. For a movie $m$, let $\mathcal{U}_{m}$ be the set of users who rated $m$ and $M_{u}$ be the set of movies rated by user $u$. For a set or tuple $A$, let $\bar{A}$ denote the average of the numbers in $A$. For a tuple of weights $\mathbf{w}=(w_{1},\ldots,w_{n})$ such that $0\leq w_{i}\leq 1$ and $\sum_{i=1}^{n}w_{i}=1$ and a tuple of positive real numbers $\mathbf{a}=(a_{1},\ldots,a_{n})$, the \textit{weighted harmonic mean} of $\mathbf{a}$ is defined as
	\begin{align*}
		\mathfrak{H}(\mathbf{a},\mathbf{w})
		& = \dfrac{\sum_{i=1}^{n}w_{i}}{\sum_{i=1}^{n}\dfrac{w_{i}}{a_{i}}}
	\end{align*}
	Usually, \textit{weighted arithmetic mean} is used to predict the ratings in a recommendation system. But in this paper, we have investigated the results using harmonic mean. Next, we describe the rating prediction algorithm for a pair $(u,m)$.
	\begin{algorithm}[H]
		\SetAlgoLined
		\KwIn{Test data in the format $(u,m)$, Threshold $t$, similarity $s$, $T$ to take first $T$ neighbors for a user $u$}
		\KwOut{A single integer in the range $[1,5]$ denoting the predicted rating}
		\KwData{Train data in the format $(u,m,r)$}
		$W\leftarrow []$\\
		$X\leftarrow []$\\
		$tot\leftarrow0$\\
		$R\leftarrow U_{m}$\\
		res = 0\\
		\For{$v\in R$}{
			\If{$|C_{uv}|<t$}{continue}
			\If{$S_{uv}<s$}{continue}
			\If{$S_{uv}\geq s$}{
				$W\leftarrow [W,s]$\\
				$X\leftarrow [X,r_{vm}]$\\
				$tot\leftarrow tot+1$\\
				\If{$tot>T$}{break}
			}
		}
		\If{$tot>0$}{
			res = $\mathfrak{H}(X, W)$
		}
		\Else{
			\If{$|M_{u}|>0$}{
				res = $\bar{M_{u}}$
			}
			\Else{
				res = $\bar{R_{m}}$
			}
		}
		res = res+$.5$\\
		res = floor(res)\\
		return res
		\caption{Algorithm to predict rating}
	\end{algorithm}
	\subsection{Algorithm Principles}
	Our algorithm is based on the following principles.
	\begin{enumerate}
		\item[\bfseries First principle] \textit{Two users $u$ and $v$ are more relevant for each other if it is ensured that $|C_{uv}|\geq t$ for some large enough positive integer $t$}. It is obvious how this principle helps us establish better correlation between users since two users $u$ and $v$ can have very high correlation with very low number of common items between them.
		\item[\bfseries Second principle] \textit{If first principle is established, then it is possible to get an accurate estimation of predicted rating with a lower number of neighbors instead of using a very large number of neighbors}. This helps us predict a rating a lot more efficiently with better accuracy. It is also possible to get a mathematical sense of why this works in practice. Let $m$ and $n$ be positive integers such that $m>n$. Let $u$ be a user and $\mathcal{N}_{m}=\{v:|C_{uv}|\geq t\}$ and $\mathcal{N}_{n}=\{u:|C_{uv}|\geq t\}$ be two sets of neighbors of a user $u$ such that $|\mathcal{C}_{m}|=m$ and $|\mathcal{N}_{n}|=n$. Using the condition $|C_{uv}|\geq t$, it can be assumed that if $s<t$ for a positive integer $s$, then
		\begin{align}
			P(\sigma^{2}(A)
			& \leq \sigma^{2}(B))\label{eqn:secprin}
		\end{align}
		should be very high where $A=\{r_{vm}:|C_{uv}|\geq t\}$, $B=\{r_{vm}:|C_{uv}|\geq s\}$ and $P(x)$ denotes the probability of the random variable $x$. Since higher value of the threshold $t$ ensures better similarity between two users, we can say in a non-rigorous way that the \textit{Pigeonhole principle} ensures \eqref{eqn:secprin} is high enough in practice more often than not.
		\item[\bfseries Third principle] \textit{If two sets of ratings $A$ and $B$ have similar variance and consist ratings given by similar users of $u$ only, then $P(|\bar{A}-\bar{B}|<\epsilon)$ is very high for some positive real number $\epsilon$ which is considerably smaller than $1$}. We can show that a special case holds very often in practice. Since $A$ and $B$ has ratings from similar users only, assume that both $A$ and $B$ consist of $4$ and $5$ only (our data set rating range is $[1,5]$). If $|A|=m$ and $|B|=n$ and $a$ is the number of $4$ in $A$ whereas $b$ is the number of $4$ in $B$, then
		\begin{align*}
			\bar{A}
			& = \dfrac{4a+5(m-a)}{m}\\
			& = \dfrac{5m-a}{m}\\
			& = 5-\dfrac{a}{m}\\
			\bar{B}
			& = \dfrac{4b+5(n-b)}{n}\\
			& = \dfrac{5n-b}{n}\\
			& = 5-\dfrac{b}{n}
		\end{align*}
		As we can see, it does not matter if $m\gg n$ or $n\gg m$ since the difference $|\bar{A}-\bar{B}|$ or the ratio $\frac{\bar{A}}{\bar{B}}$ only depends on the ratio $\dfrac{a}{m}$ and $\dfrac{b}{n}$. So as long as these ratios are similar, $\bar{A}$ and $\bar{B}$ are similar as well.
	\end{enumerate}\clearpage
	\section{Results}
	We show the results of our algorithm on the Movielens 1 million data set and backup our claims with empirical evidence below. The quality of performance is measured using the $F1$ score. The $F1$ score is the harmonic mean of precision and recall.
	\begin{align*}
		P
		& = \dfrac{TP}{TP+FP}\\
		R
		& = \dfrac{TP}{TP+FN}\\
		F1
		& = \dfrac{2PR}{R+P}
	\end{align*}
	We have chosen $F1$ as an accuracy metric for couple of reasons.
	\begin{enumerate}
		\item It is well known that the harmonic mean of $n$ positive real numbers $a_{1},\ldots,a_{n}$ is less than $\min\{a_{1},\ldots,a_{n}\}$ for $n>1$. So an $F1$ value of $x$ indicates that the model has at least $x$ precision and $x$ recall.
		\item $F1$ score can penalize bad precision or bad recall scenarios properly where arithmetic or geometric means may fail. For example, if we consider a recommendation where we do not predict anything, technically, we do not have any errors. So the precision would be $100$ percent whereas recall would be $0$ percent. An arithmetic average would give us an accuracy of $50$ percent which is clearly wrong. $F1$ score penalizes all such scenarios accordingly. For this particular example, we would still have an $F1$ score of $0$ percent.
	\end{enumerate}
	For this paper, we call a test data point \textit{true positive} if a certain movie $m$ is to be recommended to user $u$ based on the rating. If the rating is over $k$ for some fixed $k$, then the movie is to be recommended, otherwise it is to be discarded. We have showed results for both $k\geq 3$ and $k\geq 4$ in this paper.

	We have also used \textit{mean absolute percentage error} as a metric in place of the more traditional \textit{mean absolute error}. Mean absolute error is calculated using the formula:
	\begin{align*}
		MAE
		& = \dfrac{1}{n}\sum_{i=1}^{n}|y_{i}-y_{i}'|
	\end{align*}
	whereas mean absolute percentage error is calculated using the formula
	\begin{align*}
		MAPE
		& = \dfrac{1}{n}\sum_{i=1}^{n}\dfrac{|y_{i}-y_{i}'}{y_{i}}
	\end{align*}
	where $y_{i}$ is the actual value for the data point at $i$ and $y_{i}'$ is the predicted value of this data point. It is easy to see that mean absolute error puts equal weights on all ratings whereas mean absolute percentage errors mitigates that problem to some extent. So we felt it important to also show the results in terms of mean absolute percentage errors.
	\foreach \x in {10,20,30,50,100}
	{
		\foreach\y in {7,8,9}{
			\begin{center}
				\begin{figure}
					\includegraphics[scale=.4]{\x\y.jpg}
				\end{figure}
				\label{fig:\x\y}
			\end{center}
		}\clearpage
	}
	\section{Conclusion}
	As we can see in the result section, all the graphs tend to give us a flat line very early on. usually, we do not get much improvement in accuracy after $30$ neighbors and we do not have to go beyond $.7$ similarity threshold for considering neighbors.
	%\bibliographystyle{ACM-Reference-Format}
	\bibliographystyle{ACM-Reference-Format}
	\bibliography{bibfile}

\end{document}